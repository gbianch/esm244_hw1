---
title: "HW1_task2"
author: "Grace Bianchi"
date: "2023-02-02"
output: html_document
---

##Intro

This code wrangles and examines a small subset of seawater sample data from CalCOFI. Two linear regression models are used to predict oxygen saturation based on several chemical and physical variables, and compared using AIC, BIC, and cross validation. Specifically, one model examines the relationship between O2 saturation of seawater off California's coast and water temperature, salinity, and phosphate concentration; and the second model examines the relationship between oxygen saturation and water temperature, salinity, phosphate concentration, and depth.

**Data Summary from the CalCOFI site:** “Since 1949, hydrographic and biological data of the California Current System have been collected on CalCOFI cruises. The 70+ year hydrographic time-series includes temperature, salinity, oxygen and phosphate observations. In 1961, nutrient analysis expanded to include silicate, nitrate and nitrite; in 1973, chlorophyll was added; in 1984, C14 primary productivity incubations were added; and in 1993, CTD profiling began.”


**Data citation:** CalCOFI data are available for use without restriction. Data downloaded from https://calcofi.org/ccdata.html.  Accessed 1/10/2022.


```{r setup, include=TRUE, message = FALSE, warning = FALSE, echo = TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(AICcmodavg)
library(janitor)
library(here)
library(broom)
library(kableExtra)

```


```{r read in data}
# read in data
seawater_samples_data <- read_csv(here("calcofi_seawater_samples.csv"))

```

Complete Task 2 in a single well-organized .Rmd. Read in the seawater sample data (calcofi_seawaxter_samples.csv), then create and compare two multiple linear regression models:


### Model 1
Oxygen saturation as a function of water temperature, salinity, and phosphate concentration
```{r}
## multiple linear regression model 1 w Oxygen saturation as a function of water temperature, salinity, and phosphate concentration

f1 <- o2sat ~ t_deg_c + salinity + po4u_m
mdl1 <- lm(formula = f1, data = seawater_samples_data)

mdl1_summary <- summary(mdl1)

tidy(mdl1_summary) %>% 
  kable(caption = "Model 1...") %>% 
  kable_classic()


# AIC(mdl1)
# AIC: 618.3868
```


### Model 2
Oxygen saturation as a function of water temp, salinity, phosphate concentration, and depth.
```{r}
## multiple linear regression model 2 w Oxygen saturation as a function of water temp, salinity, phosphate concentration, and depth.

f2 <- o2sat ~ t_deg_c + salinity + po4u_m + depth_m
mdl2 <- lm(formula = f2, data = seawater_samples_data)

mdl2_summary <- summary(mdl2)

tidy(mdl2_summary) %>% 
  kable(caption = "Model 2...") %>% 
  kable_classic()

# AIC(mdl2)

# Residual standard error: 5.08 on 95 degrees of freedom
#Multiple R-squared:  0.9574,	Adjusted R-squared:  0.9557 
# F-statistic: 534.3 on 4 and 95 DF,  p-value: < 2.2e-16

# AIC: 615.7016
  
```

The adjusted R^2^ indicates that either model explains ~95% of the observed
Model 1 Adjusted r squared : `r mdl1_summary$adj.r.squared`

## Identify better model using following 3 methods

Method 1 - AICc (in the AICcmodavg package, either AICc() or aictab()) 

```{r}
AIC(mdl1, mdl2)
#     df  AIC
# mdl1	5	618.3868		
# mdl2	6	615.7016

AICc(mdl1) # 619.0251

AICc(mdl2) # 616.6048

aictab(list(mdl1, mdl2))

#     K   AICc Delta_AICc AICcWt Cum.Wt      LL
# Mod2 6 616.60       0.00   0.77   0.77 -301.85
# Mod1 5 619.03       2.42   0.23   1.00 -304.19

```
From this, the second model is the"best


Method 2 - BIC
```{r}
bictab(list(mdl1, mdl2))

#      K    BIC Delta_BIC BICWt Cum.Wt      LL
# Mod2 6 631.33      0.00  0.51   0.51 -301.85
# Mod1 5 631.41      0.08  0.49   1.00 -304.19
```


Method 3 - Ten-Fold Cross Validation
Perform a ten-fold cross validation on the two models, using root-mean-square error as the scoring method. Remember that your final model should be trained on the full dataset, not any of the folds.  You may use a for loop, or purrr, or tidymodels method, whichever you prefer.
If your methods indicate different models as the best one, identify which model you would use and explain your reasoning.

```{r eval = FALSE}
set.seed(123)

n_folds <- 10
fold_vec <- rep(1:n_folds, length.out = nrow(seawater_samples_data))
seawater_kfold <- seawater_samples_data %>%
  mutate(fold = sample(fold_vec, size = n(), replace = FALSE))

results_df <- data.frame()
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  return(mean(accurate, na.rm = TRUE))
}


for(i in 1:n_folds) {
  kfold_test <- seawater_kfold %>%
    filter(fold == i)
  kfold_train <- seawater_kfold %>%
    filter(fold != i)
  
  kfold_blr1 <- glm(f1, data = kfold_train, family = 'binomial')
  kfold_blr2 <- glm(f2, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr1 = predict(kfold_blr1, kfold_test, type = 'response'),
           blr2 = predict(kfold_blr2, ., type = 'response')) %>%
    mutate(pred1 = ifelse(blr1 > 0.50, 'Chinstrap', 'Adelie'),
           pred2 = ifelse(blr2 > 0.50, 'Chinstrap', 'Adelie'))
  kfold_accuracy <- kfold_pred %>%
    summarize(blr1_acc = pred_acc(species, pred1),
              blr2_acc = pred_acc(species, pred2))
  
  results_df <- bind_rows(results_df, kfold_accuracy)
}


results_df %>%
  summarize(blr1_acc = mean(blr1_acc),
            blr2_acc = mean(blr2_acc))

```

```{r eval = FALSE}
# function to calculate accuracy, given a "truth" vector and "prediction" vector
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  return(mean(accurate, na.rm = TRUE))
}

# function to calculate accuracy of BLR of one fold (training and testing)
calc_fold <- function(i, fold_df, f) {
  kfold_test <- fold_df %>%
    filter(fold == i)
  kfold_train <- fold_df %>%
    filter(fold != i)
  
  kfold_blr <- glm(f, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr = predict(kfold_blr, kfold_test, type = 'response')) %>%
    mutate(pred = ifelse(blr > 0.50, 'Chinstrap', 'Adelie'))
  
  kfold_accuracy <- kfold_pred %>%
    summarize(blr_acc = pred_acc(species, pred)) # using my other function
  
  return(kfold_accuracy)
}

n_folds <- 10

results1_purrr_df <- purrr::map(.x = 1:n_folds, # sequence of fold numbers
                                .f = calc_fold, # function
                                fold_df = ad_chin_kfold, # additional argument to calc_fold()
                                f = f1) %>%              # additional argument to calc_fold()
  bind_rows() %>%
  mutate(mdl = 'f1')

results2_purrr_df <- purrr::map(.x = 1:n_folds, .f = calc_fold, 
                               fold_df = ad_chin_kfold,
                               f = f2) %>%
  bind_rows() %>%
  mutate(mdl = 'f2')

results_purrr_df <- bind_rows(results1_purrr_df, results2_purrr_df) %>%
  group_by(mdl) %>%
  summarize(mean_acc = mean(blr_acc))

results_purrr_df
```

Challenge yourself (optional!) - try one or both of these:
Explore other possible models - there may be other combinations of the available variables that provide an even better-performing model than these two.  If you identify a better performing model, report the AIC and cross validation results for your new model, along with the results for the two models listed above.  
In the K-fold cross validation, create a nested for-loop (a for loop inside another for loop) to perform multiple iterations of your 10-fold cross validation, with different random folds assigned for each iteration.  Make sure to use a different indexing variable (e.g., the outer loop might start for(j in 1:n_iterations) while the inner loop might start for(i in 1:n_folds))

